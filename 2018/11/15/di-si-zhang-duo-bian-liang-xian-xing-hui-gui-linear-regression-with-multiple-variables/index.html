<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="第四章 多变量线性回归(Linear Regression with Multiple Variables), 江南小虫虫的博客">
    <meta name="description" content="Multiple Features（多维特征）
 在这段视频中 我们将开始 介绍一种新的 更为有效的线性回归形式 这种形式适用于多个变量或者多特征量的情况


在之前我们学习过的 线性回归中 我们只有一个单一特征量 房屋面积 x 我们希望用">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>第四章 多变量线性回归(Linear Regression with Multiple Variables) | 江南小虫虫的博客</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <script src="/libs/jquery/jquery.min.js"></script>
    
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper head-container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">江南小虫虫的博客</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/biaobai/表白.html" class="waves-effect waves-light">
      
      <i class="fas fa-heart" style="zoom: 0.6;"></i>
      
      <span>给亲爱的老婆仔</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">江南小虫虫的博客</div>
        <div class="logo-desc">
            
            尽力就行~~~
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/biaobai/表白.html" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-heart"></i>
			
			给亲爱的老婆仔
		</a>
          
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/10.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">第四章 多变量线性回归(Linear Regression with Multiple Variables)</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Linear-Regression/">
                                <span class="chip bg-color">Linear-Regression</span>
                            </a>
                        
                            <a href="/tags/多变量线性回归/">
                                <span class="chip bg-color">多变量线性回归</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/机器学习/" class="post-category">
                                机器学习
                            </a>
                        
                            <a href="/categories/机器学习/机器学习入门/" class="post-category">
                                机器学习入门
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2018-11-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2019-11-30
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                
				
                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
            
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Multiple-Features（多维特征）"><a href="#Multiple-Features（多维特征）" class="headerlink" title="Multiple Features（多维特征）"></a>Multiple Features（多维特征）</h2><blockquote>
<p> 在这段视频中 我们将开始 介绍一种新的 <strong>更为有效的线性回归形式 这种形式适用于多个变量或者多特征量的情况</strong></p>
</blockquote>
<ul>
<li>在之前我们学习过的 线性回归中 我们只有一个单一特征量 房屋面积 x 我们希望用这个特征量 来预测 房子的价格 这就是我们的假设</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603422_2029872331_1540481948" alt="_1526603422_2029872331_1540481948_1526603422_2029872331.png"></p>
<a id="more"></a>
<ul>
<li>但是想象一下 如果我们<strong>不仅有房屋面积 作为预测房屋 价格的特征量 或者变量 我们还知道 卧室的数量 楼层的数量以及房子的使用年限</strong> 这样就给了我们 更多可以用来 预测房屋价格的信息</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603455_433733145_1540741332" alt="_1526603455_433733145_1540741332_1526603455_433733145.png"></p>
<ul>
<li>先简单介绍一下记法 我们开始的时候就提到过 我要用 x 下标1 x 下标2 等等 来表示 这种情况下的四个特征量 然后仍然用 Y来表示我们 所想要预测的输出变量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603471_617319148_1540741358" alt="_1526603471_617319148_1540741358_1526603471_617319148.png"></p>
<ul>
<li>让我们来看看更多的表示方式 现在我们有四个特征量 我要用<code>小写n</code> 来表示<strong>特征量的数目</strong> 因此在这个例子中 我们的n等于4 因为你们看 我们有 1 2 3 4 共4个特征量 这里的n和我们之前 使用的n不同 之前我们是用的“<code>m</code>”来<strong>表示样本的数量</strong> 所以如果你有47行 那么m就是这个表格里面的行数 或者说是训练样本数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603493_859265827_1540741432" alt="_1526603493_859265827_1540741432_1526603493_859265827.png"></p>
<ul>
<li>然后我要用<code>x 上标 (i)</code> 来表示<strong>第i个 训练样本的 输入特征值</strong> 举个具体的例子来说 <code>x上标 (2)</code> 就是表示<strong>第二个 训练样本的特征向量</strong> 因此这里 x(2)就是向量 [1416, 3, 2, 40] 因为这四个数字对应了 我用来预测房屋价格的 第二个房子的 四个特征量 因此在这种记法中 这个上标2 就是训练集的一个索引 而不是x的2次方 这个2就对应着 你所看到的表格中的第二行 即我的第二个训练样本</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104203747.png" alt></p>
<ul>
<li>x上标(2) 这样表示 就是一个四维向量 事实上更普遍地来说 这是n维的向量 用这种表示方法 x上标2就是一个向量 因此 我用<code>x上标(i) 下标j</code> 来表示 <strong>第i个训练样本的 第j个特征量</strong> 因此具体的来说 <code>x上标(2)下标3</code>代表着 <strong>第2个训练样本里的第3个特征量</strong> 对吧？ 这个是3 我写的不太好看 所以说x上标(2)下标3就等于2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204137.png" alt></p>
<ul>
<li>既然我们有了多个特征量 让我们继续讨论一下 我们的假设形式应该是怎样的 这是我们之前使用的假设形式 x就是我们唯一的特征量 但现在我们有了多个特征量 我们就不能再 使用这种简单的表示方式了 取而代之的 我们将把线性回归的假设 改成这样 θ0加上 θ1 乘以 x1 加上 θ2乘以x2 加上 θ3 乘以x3 加上θ4乘以x4 然后如果我们有n个特征量 那么我们要将所有的n个特征量相加 而不是四个特征量 我们需要对n个特征量进行相加</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204157.png" alt></p>
<ul>
<li>举个具体的例子 在我们的设置的参数中 我们可能有h(x)等于 <code>80 + 0.1 x1 + 0.01x2 + 3x3 - 2x4</code> 这就是一个 假设的范例 别忘了 假设是为了预测 大约以千刀为单位的房屋价格 就是说 一个房子的价格 可以是 80 k加上 0.1乘以x1 也就是说 每平方尺100美元 然后价格 会随着楼层数的增加 再继续增长 x2是楼层数 接着价格会继续增加 随着卧室数的增加 因为x3是 卧室的数量 但是呢 房子的价格会 随着使用年数的增加 而贬值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204236.png" alt></p>
<ul>
<li>这是重新改写过的假设的形式 接下来 我要来介绍一点 简化这个等式的表示方式</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204258.png" alt></p>
<ul>
<li>为了表示方便 我要将<code>x下标0的值设为1</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204355.png" alt></p>
<ul>
<li>具体而言 这意味着 对于第i个样本 都有一个向量x上标(i) 并且x上标(i) 下标0等于1 你可以认为我们 定义了一个额外的第0个特征量 因此 我过去有n个特征量 因为我们有x1 x2 直到xn 由于我另外定义了 额外的第0个特征向量 并且它的取值 总是1 所以我现在的特征向量x 是一个从0开始标记的 n+1维的向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204411.png" alt></p>
<ul>
<li>所以现在就是一个 n+1维的特征量向量 但我要从0开始标记 同时 我也想把我的参数 都看做一个向量 所以我们的参数就是 我们的<code>θ0 θ1 θ2 等等</code> 直到θn 我们要把 所有的参数都写成一个向量 <code>θ0 θ1...</code>一直到 直到θn 这里也有一个从0开始标记的矢量 下标从0开始 这是另外一个</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204425.png" alt></p>
<ul>
<li>所以我的假设 现在可以写成θ0乘以x0 加上θ1乘以x1直到 θn 乘以xn 这个等式 和上面的等式是一样的 因为你看 x0等于1</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204451.png" alt></p>
<ul>
<li>下面 我要 把这种形式<code>假设等式</code> 写成 <code>θ转置乘以X</code> 取决于你对 向量内积有多熟悉 如果你展开 θ转置乘以X 那么就得到 θ0 θ1直到θn 这个就是θ转置 实际上 这就是一个 n+1乘以1维的矩阵 也被称为行向量 用行向量 与X向量相乘 X向量是 x0 x1等等 直到xn 因此内积就是 θ转置乘以X 就等于这个等式</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204531.png" alt></p>
<ul>
<li>这就为我们提供了一个 表示假设的 更加便利的形式 即<strong>用参数向量θ以及 特征向量X的内积</strong> 这就是改写以后的 表示方法 这样的表示习惯 就让我们 可以以这种紧凑的形式写出假设 这就是多特征量情况下的假设形式 起另一个名字 就是 所谓的<code>多元线性回归</code></li>
<li><code>多元</code>一词 也就是<strong>用来预测的多个特征量 或者变量 就是一种更加好听的说法</strong>罢了</li>
</ul>
<h2 id="Gradient-Descent-for-Multiple-Variables-多变量梯度下降"><a href="#Gradient-Descent-for-Multiple-Variables-多变量梯度下降" class="headerlink" title="Gradient Descent for Multiple Variables(多变量梯度下降)"></a>Gradient Descent for Multiple Variables(多变量梯度下降)</h2><blockquote>
<p>在之前的视频中 我们谈到了一种线性回归的假设形式 这是一种有多特征或者是多变量的形式 在本节视频中 我们将会谈到如何找到满足这一假设的<strong>参数</strong> 尤其是<strong>如何使用梯度下降法 来解决多特征的线性回归问题</strong></p>
</blockquote>
<ul>
<li>为尽快让你理解 现假设现有多元线性回归 并约定 x0=1 该模型的参数是从 θ0 到 θn 不要认为这是 n+1 个单独的参数 你可以把这 n+1 个 θ 参数想象成一个 n+1 维的向量 θ 所以 你现在就可以把这个模型的参数 想象成其本身就是一个<code>n+1 维的向量</code> 我们的代价函数是从 θ0 到 θn 的函数 J 并给出了误差项平方的和 但同样地 不要把函数 J 想成是一个关于 n+1 个自变量的函数 而是看成带有一个 <code>n+1 维向量的函数</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204601.png" alt></p>
<ul>
<li>这就是梯度下降法 我们将会<strong>不停地用 θj 减去 α 倍的导数项 来替代 θj</strong> 同样的方法 我们写出函数J(θ) 因此 θj 被更新成 θj 减去学习率 α 与对应导数的乘积 就是代价函数的对参数 θj 的偏导数 当我们实现梯度下降法后 你可以仔细观察一下 尤其是它的偏导数项</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204617.png" alt></p>
<ul>
<li>下面是我们当特征 n=1 时 梯度下降的情况 我们有两条针对参数 θ0 和 θ1 不同的更新规则 希望这些对你来说并不陌生 这一项是代价函数里部分求导的结果 就是<strong>代价函数相对于 θ0 的偏导数</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204640.png" alt></p>
<ul>
<li>同样 对参数 θ1 我们有另一个更新规则  仅有的一点区别是 当我们之前只有一个特征 我们称该特征为x(i) 但现在我们在新符号里 我们会标记它为 x 上标 (i) 下标1 来表示我们的特征 以上就是当我们仅有一个特征时候的算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204701.png" alt></p>
<ul>
<li>下面我们来讲讲<strong>当有一个以上特征时候的算法</strong> 现有数目远大于1的很多特征 我们的梯度下降更新规则变成了这样 有些同学可能知道微积分 如果你看看代价函数 <strong>代价函数 J 对参数 θj 求偏导数</strong> 你会发现 求其偏导数的那一项 我已经用蓝线圈出来了 如果你实现了这一步 你将会得到多元线性回归的梯度下降算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204716.png" alt></p>
<ul>
<li>最后 我想让你明白 为什么新旧两种算法实际上是一回事儿 或者说为什么这两个是类似的算法 为什么它们都是梯度下降算法 考虑这样一个情况 有两个或以上个数的特征 同时我们有对θ1、θ2、θ3的三条更新规则 当然可能还有其它参数 如果你观察θ0的更新规则  你会发现这跟之前 n=1的情况相同 它们之所以是等价的 这是因为在我们的标记约定里有 x(i)0=1 也就是 我用品红色圈起来的两项是等价的 同样地 如果你观察 θ1 的更新规则 你会发现这里的这一项是 和之前对参数θ1的更新项是等价的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204732.png" alt></p>
<ul>
<li>在这里我们只是用了新的符号x(i)1来表示我们的第一个特征 现在我们有个更多的特征 那么就可以用与之前相同的更新规则 我们可以用同样的规则来处理 θ2 等其它参数 这张幻灯片的内容不少 请务必仔细理解 如果觉得幻灯片上数学公式没看懂 尽管暂停视频 请确保理解了再继续后面的学习 如果你将这些算法都实现了 那么你就可以直接应用到多元线性回归中了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204746.png" alt></p>
<h3 id="多变量梯度下降笔记"><a href="#多变量梯度下降笔记" class="headerlink" title="多变量梯度下降笔记"></a>多变量梯度下降笔记</h3><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204804.png" alt></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204811.png" alt></p>
<h2 id="Gradient-Descent-in-Practice-1-Feature-Sacling-梯度下降法实践1-特征缩放"><a href="#Gradient-Descent-in-Practice-1-Feature-Sacling-梯度下降法实践1-特征缩放" class="headerlink" title="Gradient Descent in Practice 1-Feature Sacling(梯度下降法实践1-特征缩放)"></a>Gradient Descent in Practice 1-Feature Sacling(梯度下降法实践1-特征缩放)</h2><blockquote>
<p>在这段视频 以及下一段视频中 我想告诉你一些关于 梯度下降运算中的实用技巧 在这段视频中 我会告诉你一个称为<code>特征缩放 (feature scaling)</code> 的方法</p>
</blockquote>
<ul>
<li>如果你有一个机器学习问题 这个问题有多个特征 如果你能确保这些特征 都处在一个相近的范围 我的意思是<strong>确保不同特征的取值 在相近的范围内,这样梯度下降法就能更快地收敛</strong></li>
<li>具体地说 假如你有一个具有两个特征的问题 其中 <code>x1</code> 是房屋面积大小 它的取值 在<code>0到2000</code>之间 <code>x2</code> 是卧室的数量 可能这个值 取值范围在<code>1到5</code>之间 如果你画出代价函数 J(θ) 的轮廓图 那么这个轮廓看起来 应该是像这样的 J(θ) 是一个关于 参数 θ0 θ1 和 θ2 的函数 但我要忽略 θ0 所以暂时不考虑 θ0 并假想一个函数的变量 只有 θ1 和 θ2 但如果 x1 的取值范围 远远大于 x2 的取值范围的话 那么最终画出来的 代价函数 J(θ) 的轮廓图 就会呈现出这样一种 <strong>非常偏斜 并且椭圆的形状 2000 和 5的比例</strong> 会让这个椭圆更加瘦长 所以 这是一个又瘦又高的 椭圆形轮廓图 就是这些非常高大细长的椭圆形 构成了代价函数 J(θ)</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204851.png" alt></p>
<ul>
<li>而如果你用这个代价函数 来运行梯度下降的话 你要得到梯度值 <strong>最终可能 需要花很长一段时间 并且可能会来回波动 然后会经过很长时间 最终才收敛到<code>全局最小值</code></strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204920.png" alt></p>
<ul>
<li>事实上 你可以想像 如果这些 轮廓再被放大一些的话 如果你画的再夸张一些 把它画的更细更长 那么可能情况会更糟糕 梯度下降的过程 可能更加缓慢 需要花更长的时间 反复来回振荡 最终才找到一条正确通往全局最小值的路</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204943.png" alt></p>
<ul>
<li>在这样的情况下 一种有效的方法是进行<code>特征缩放(feature scaling)</code> 具体来说 把特征 x 定义为 房子的面积大小 除以2000的话 并且把 x2 定义为 卧室的数量除以5 那么这样的话 表示代价函数 J(θ) 的轮廓图的形状 就会变得偏移没那么严重 可能看起来更圆一些了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204956.png" alt></p>
<ul>
<li>如果你用这样的代价函数 来执行梯度下降的话 那么 梯度下降算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205010.png" alt></p>
<ul>
<li>你可以从数学上来证明 梯度下降算法 就会找到一条 更捷径的路径通向全局最小 而不是像刚才那样 沿着一条让人摸不着头脑的路径 一条复杂得多的轨迹 来找到全局最小值 因此 通过特征缩放 通过”消耗掉”这些值的范围 在这个例子中 我们最终得到的两个特征 x1 和 x2 都在<code>0和1之间</code> 这样你得到的梯度下降算法 就会更快地收敛</li>
<li>更一般地 我们执行特征缩放时 我们通常的目的是 <strong>将特征的取值约束到 <code>-1 到 +1</code> 的范围内</strong> 你的特征 x0 是总是等于1 因此 这已经是在这个范围内 但<strong>对其他的特征 你可能需要通过<code>除以</code>不同的数 来让它们处于同一范围内</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205024.png" alt></p>
<ul>
<li><strong>-1 和 +1 这两个数字并不是太重要</strong> 所以 如果你有一个特征  x1 它的取值 在0和3之间 这没问题 如果你有另外一个特征 取值在-2 到 +0.5之间 这也没什么关系 这也非常接近 -1 到 +1的范围 这些都可以</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205036.png" alt></p>
<ul>
<li>但如果你有另一个特征 比如叫 x3 假如它的范围 在 -100 到 +100之间 那么 这个范围 跟-1到+1就有很大不同了 所以 这可能是一个 不那么好的特征 类似地 如果你的特征在一个 非常非常小的范围内 比如另外一个特征 x4 它的范围在 0.0001和+0.0001之间 那么 这同样是一个 比-1到+1小得多的范围 比-1到+1小得多的范围 因此 我同样会认为这个特征也不太好</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205059.png" alt></p>
<ul>
<li>所以 可能你认可的范围 也许可以大于 或者小于 -1 到 +1 但是也别太大 只要大得不多就可以接受 比如 +100 或者也别太小 比如这里的0.001 不同的人有不同的经验 但是我一般是这么考虑的 <strong>如果一个特征是在 <code>-3 到 +3</code> 的范围内 那么你应该认为 这个范围是可以接受的</strong> 但如果这个范围 大于了 -3 到 +3 的范围 我可能就要开始注意了 如果它的取值 在-1/3 到+1/3的话 我觉得 还不错 可以接受 或者是0到1/3 或-1/3到0 这些典型的范围 我都认为是可以接受的 但如果特征的范围 取得很小的话 比如像这里的 x4 你就要开始考虑进行特征缩放了</li>
<li>因此 总的来说 不用过于担心 你的特征是否在完全 相同的范围或区间内 但是只要他们都 <strong>只要它们足够接近的话 梯度下降法就会正常地工作</strong></li>
<li>除了在特征缩放中 将特征除以最大值以外 有时候我们也会进行一个 称为<code>均值归一化的工作(mean normalization)</code> 我的意思是这样的 如果你有一个特征 <code>xi</code> 你就用 <code>xi - μi</code> 来替换 通过这样做 <strong>让你的特征值 具有为0的平均值</strong> 很明显 我们不需要 把这一步应用到 x0中 因为 x0 总是等于1的 所以它不可能有 为0的的平均值</li>
<li>但是 对其他的特征来说 比如房子的大小 取值介于0到2000 并且假如 房子面积 的平均值 是等于1000的 那么你可以用这个公式 将 x1 的值变为 x1 减去平均值 μ1 再除以2000 类似地 如果你的房子有 五间卧室 并且平均一套房子有 两间卧室 那么你可以 使用这个公式 来归一化你的第二个特征 x2 在这两种情况下 你可以算出新的特征 x1 和 x2 这样它们的范围 可以在-0.5和+0.5之间 当然这肯定不对 x2的值实际上肯定会大于0.5 但很接近</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205132.png" alt></p>
<ul>
<li>更一般的规律是 你可以用这样的公式 你可以用 <code>(x1 - μ1)/S1</code> 来替换原来的特征 x1 其中定义<code>μ1</code>的意思是 在训练集中 <strong>特征 x1 的平均值</strong> 而 <code>S1</code> 是 该特征值的范围 我说的范围是指 <strong>最大值减去最小值</strong>或者学过 标准差的同学可以记住 也可以把 S1 设为 变量的<code>标准差</code> 但其实用最大值减最小值就可以了 类似地 对于第二个 特征 x2 你也可以用同样的这个 特征减去平均值 再除以范围 来替换原特征 范围的意思依然是最大值减最小值 这类公式将 把你的特征 变成这样的范围 也许不是完全这样 但大概是这样的范围</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205148.png" alt></p>
<ul>
<li>顺便提一下 有些同学可能比较仔细 如果我们用最大值减最小值 来表示范围的话 这里的5有可能应该是4 如果最大值为5 那么减去最小值1 这个范围值就是4 但不管咋说 这些取值 都是非常近似的 <strong>只要将特征转换为 相近似的范围 就都是可以的</strong> <strong>特征缩放其实 并不需要太精确 只是为了让梯度下降 能够运行得更快一点而已</strong></li>
<li>好的 现在你知道了 什么是特征缩放 通过使用这个简单的方法 你可以将梯度下降的速度变得更快 让梯度下降收敛所需的循环次数更少 这就是特征缩放 在接下来的视频中 我将介绍另一种技巧来使梯度下降 在实践中工作地更好</li>
</ul>
<h2 id="Gradient-Descent-in-Practice-II-Learning-Rate-梯度下降法实践2-学习率"><a href="#Gradient-Descent-in-Practice-II-Learning-Rate-梯度下降法实践2-学习率" class="headerlink" title="Gradient Descent in Practice II-Learning Rate(梯度下降法实践2-学习率)"></a>Gradient Descent in Practice II-Learning Rate(梯度下降法实践2-学习率)</h2><blockquote>
<p>在本段视频中 我想告诉大家 一些关于梯度下降算法的实用技巧 我将集中讨论 <code>学习率 α</code> 具体来说 这是梯度下降算法的 更新规则 这里我想要 告诉大家 如何调试 也就是我认为应该如何确定 梯度下降是正常工作的 此外我还想告诉大家 如何选择学习率 α 也就是我平常 如何选择这个参数 我通常是怎样确定 梯度下降正常工作的</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205201.png" alt></p>
<ul>
<li><strong>梯度下降算法所做的事情 就是为你找到 一个 θ 值 并希望它能够最小化代价函数 J(θ) *<em>我通常会在 梯度下降算法运行时 绘出代价函数 J(θ) 的值 这里的 <code>x 轴</code>是表示 *</em>梯度下降算法的 迭代步数</strong> 你可能会得到 这样一条曲线 注意 这里的 x 轴 是迭代步数 在我们以前看到的 J(θ) 曲线中 x 轴 也就是横轴 曾经用来表示参数 θ 但这里不是 具体来说 这一点的含义是这样的 当我运行完100步的梯度下降迭代之后 无论我得到 什么 θ 值 总之 100步迭代之后 我将得到 一个 θ 值 根据100步迭代之后 得到的这个 θ 值 我将算出 代价函数 J(θ) 的值 而这个点的垂直高度就代表 梯度下降算法 100步迭代之后 得到的 θ 算出的 J(θ) 值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205220.png" alt></p>
<ul>
<li>而这个点 则是梯度下降算法 迭代200次之后 得到的 θ 算出的 J(θ) 值 所以这条曲线 显示的是 梯度下降算法迭代过程中代价函数 J(θ) 的值 <strong>如果梯度下降算法 正常工作 那么每一步迭代之后 J(θ) 都应该下降</strong> 这条曲线 的一个用处在于 它可以告诉你 如果你看一下 我画的这条曲线 当你达到 300步迭代之后 也就是300步到400步迭代之间 就是曲线的这一段 看起来 J(θ) 并没有下降多少 所以当你 到达400步迭代时 这条曲线看起来已经很平坦了 也就是说 在这里400步迭代的时候 梯度下降算法 基本上已经收敛了 因为代价函数并没有继续下降 所以说 看这条曲线 可以帮助你判断 梯度下降算法是否已经收敛</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205248.png" alt></p>
<ul>
<li>顺便说一下 对于每一个特定的问题 梯度下降算法所需的迭代次数 可以相差很大 也许对于某一个问题 梯度下降算法 只需要30步迭代就可以收敛 然而换一个问题 也许梯度下降算法就需要3000步迭代 对于另一个机器学习问题 则可能需要三百万步迭代 实际上 我们很难提前判断</li>
<li><strong>梯度下降算法 需要多少步迭代才能收敛</strong> 通常我们需要画出这类曲线 <strong>画出代价函数随迭代步数数增加的变化曲线</strong> 通常 我会通过看这种曲线 来试着判断 梯度下降算法是否已经收敛 另外 也可以 进行一些<strong>自动的收敛测试</strong> 也就是说用一种算法 来告诉你梯度下降算法 是否已经收敛 自动收敛测试 一个非常典型的例子是 如果代价函数 J(θ) 的下降小于 一个很小的值 ε 那么就认为已经收敛 比如可以选择 1e-3 但我发现 通常要选择一个合适的阈值 ε 是相当困难的 因此 为了检查 梯度下降算法是否收敛 我实际上还是 通过看 左边的这条曲线图 而不是依靠自动收敛测试</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205302.png" alt></p>
<ul>
<li>此外 这种曲线图 也可以 在算法没有正常工作时 提前警告你 具体地说 如果代价函数 J(θ) 随迭代步数 的变化曲线是这个样子 J(θ) 实际上在不断上升 那就很明显的表示 梯度下降算法没有正常工作 而这样的曲线,通常意味着你应该使用<strong>较小的学习率a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205314.png" alt></p>
<ul>
<li>这样的曲线图表示你的学习率a太大了,因此得到的结果也会不断跳过最小值,变得越来越大,因此应该使用<strong>较小的学习率a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205324.png" alt></p>
<ul>
<li>这种图也是要选择<strong>小的a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205332.png" alt></p>
<ul>
<li>a不可以太大,也不可以太小</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205430.png" alt></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205448.png" alt></p>
<ul>
<li>一般尝试的a值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205502.png" alt></p>
<h2 id="Features-and-Polynomial-Regression-特征和多项式回归"><a href="#Features-and-Polynomial-Regression-特征和多项式回归" class="headerlink" title="Features and Polynomial Regression 特征和多项式回归"></a>Features and Polynomial Regression 特征和多项式回归</h2><blockquote>
<p>你现在了解了多变量的线性回归 在本段视频中 我想告诉你 一些<strong>用来 选择特征的方法</strong>以及 <strong>如何得到不同的学习算法</strong> 当选择了合适的特征后 这些算法往往是非常有效的 另外 我也想 给你们讲一讲<code>多项式回归</code> 它使得你们能够<strong>使用 线性回归的方法来拟合 非常复杂的函数 甚至是非线性函数</strong></p>
</blockquote>
<ul>
<li>以预测房价为例 假设你有两个特征 分别是房子<code>临街的宽度</code>和<code>垂直宽度</code> 这就是我们想要卖出的房子的图片 临街宽度 被定义为这个距离 其实就是它的宽度 或者说是 你拥有的土地的宽度 如果这块地都是你的的话 而这所房子的 纵向深度就是 你的房子的深度 这是正面的宽度 这是深度 我们称之为临街宽度和纵深 你可能会 像这样 建立一个 线性回归模型 其中临街宽度 是你的第一个特征x1 纵深是你的第二个 特征x2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205534.png" alt></p>
<ul>
<li>但当我们在 运用线性回归时 你不一定非要直接用 给出的 x1 和 x2 作为特征 其实你<strong>可以自己创造新的特征</strong> 因此 如果我要预测 房子的价格 我真正要需做的 也许是 确定真正能够决定 我房子大小 或者说我土地大小 的因素是什么 因此 我可能会创造一个新的特征 我称之为 x 它是临街宽度与纵深的乘积 这是一个乘法符号 它是临街宽度与纵深的乘积 这得到的就是我拥有的土地的面积 然后 我可以把 假设选择为 使其只使用 一个特征 也就是我的 土地的面积 对吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205550.png" alt></p>
<ul>
<li>由于矩形面积的 计算方法是 矩形长和宽相乘 因此 这取决于 你从什么样的角度 去审视一个特定的问题 而不是 只是直接去使用临街宽度和纵深 这两个我们只是碰巧在开始时 使用的特征 有时 通过定义 新的特征 你确实会得到一个更好的模型</li>
<li>与选择特征的想法 密切相关的一个概念 被称为<code>多项式回归(polynomial regression)</code> 比方说 你有这样一个住房价格的数据集 为了拟合它 可能会有多个不同的模型供选择 其中一个你可以选择的是像这样的<code>二次模型</code> 因为直线似乎并不能很好地拟合这些数据 因此 也许你会想到 用这样的二次模型去拟合数据 你可能会考量 是关于价格的一个二次函数 也许这样做 会给你一个 像这样的拟合结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205609.png" alt></p>
<ul>
<li>但是 然后你可能会觉得 二次函数的模型并不好用 因为 一个二次函数最终 会降回来 而我们并不认为 房子的价格在高到一定程度后 会下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205635.png" alt></p>
<ul>
<li>因此 也许我们会 选择一个不同的多项式模型 并转而选择使用一个 <code>三次函数</code> 在这里 现在我们有了一个三次的式子 我们用它进行拟合 我们可能得到这样的模型 也许这条绿色的线 对这个数据集拟合得更好 因为它不会在最后下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205715.png" alt></p>
<ul>
<li>那么 我们到底应该如何将模型与我们的数据进行拟合呢？ 使用<code>多元线性回归的方法</code> 我们可以 通过将我们的算法做一个非常简单的修改来实现它 按照我们以前假设的形式 我们知道如何对 这样的模型进行拟合 其中 <code>ħθ(x)</code> 等于 <code>θ0 +θ1×x1 + θ2×x2 + θ3×x3</code> 那么 如果我们想 拟合这个三次模型 就是我用绿色方框框起来的这个 现在我们讨论的是 为了预测一栋房子的价格 我们用 θ0 加 θ1 乘以房子的面积 加上 θ2 乘以房子面积的平方 因此 这个式子与那个式子是相等的 然后再加 θ3 乘以 房子面积的立方 为了将这两个定义 互相对应起来 为了做到这一点 我们自然想到了 将 x1 特征设为 房子的面积 将第二个特征 x2 设为 房屋面积的平方 将第三个特征 x3 设为 房子面积的立方</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205728.png" alt></p>
<ul>
<li>那么 仅仅通过将 这三个特征这样设置 然后再应用线性回归的方法 我就可以拟合 这个模型 并最终 将一个三次函数拟合到我的数据上 我还想再说一件事 那就是 如果你像这样选择特征 那么<code>特征的归一化</code> 就变得更重要了 因此 如果 房子的大小范围在 <code>1到1000</code>之间 那么 比如说 从1到1000平方尺 那么 房子面积的平方 的范围就是 <code>一到一百万</code> 也就是 1000的平方 而你的第三个特征 x的立方 抱歉 你的第三个特征 x3 它是房子面积的 立方 范围会扩大到 1到<code>10的9次方</code> 因此 <strong>这三个特征的范围 有很大的不同 因此 如果你使用梯度下降法 应用<code>特征值的归一化</code>是非常重要的</strong> 这样才能将他们的 值的范围变得具有可比性</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205743.png" alt></p>
<ul>
<li>最后 这里是最后一个例子 关于如何使你 真正选择出要使用的特征 此前我们谈到 一个像这样的二次模型 并不是理想的 因为 你知道 也许一个二次模型能很好地拟合 这个数据 但二次 函数最后会下降 这是我们不希望的 就是住房价格往下走 像预测的那样 出现房价的下降 但是 除了转而 建立一个三次模型以外 你也许有其他的选择 特征的方法 这里有很多可能的选项 但是给你另外一个 合理的选择的例子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205757.png" alt></p>
<ul>
<li>另一种合理的选择 可能是这样的 一套房子的价格是 <code>θ0 加 θ1 乘以 房子的面积 然后 加 θ2 乘以房子面积的平方根</code> 可以吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205805.png" alt></p>
<ul>
<li>平方根函数是 这样的一种函数 也许θ1 θ2 θ3 中会有一些值 会捕捉到这个模型 从而使得这个曲线看起来 是这样的 趋势是上升的 但慢慢变得 平缓一些 而且永远不会 下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205813.png" alt></p>
<ul>
<li>因此 通过深入地研究 在这里我们研究了平方根 函数的形状 并且 更深入地了解了选择不同特征时数据的形状 有时可以得到更好的模型 在这段视频中 我们探讨了多项式回归 也就是 如何将一个 多项式 如一个二次函数 或一个三次函数拟合到你的数据上 除了这个方面 我们还讨论了 在使用特征时的选择性 例如 我们不使用 房屋的临街宽度和纵深 也许 你可以 把它们乘在一起 从而得到 房子的土地面积这个特征 实际上 这似乎有点 难以抉择 这里有这么多 不同的特征选择 我该如何决定使用什么特征呢</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205823.png" alt></p>
<ul>
<li>在之后的课程中 我们将 探讨一些算法 它们能够 自动选择要使用什么特征 因此 你可以使用一个算法 观察给出的数据 并自动为你选择 到底应该选择 一个二次函数 或者一个三次函数 还是别的函数 但是 在我们 学到那种算法之前 现在我希望你知道 你需要选择 使用什么特征 并且通过设计不同的特征 你能够用更复杂的函数 去拟合你的数据 而不是只用 一条直线去拟合 特别是 你也可以使用多项式 函数 有时候 通过采取适当的角度来观察 特征就可以 得到一个更符合你的数据的模型</li>
</ul>
<h2 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation 正规方程"></a>Normal Equation 正规方程</h2><blockquote>
<p>在这段视频中 我们要讲 <code>正规方程 (Normal Equation)</code>对于某些线性回归问题 用<strong>正规方程法求解参数 θ 的最优值</strong>更好</p>
</blockquote>
<ul>
<li>具体而言 到目前为止 我们一直在使用的<code>线性回归</code>的算法 是梯度下降法 就是说 为了最小化代价函数 J(θ) 来最小化这个 我们使用的迭代算法 需要经过很多步  也就是说通过多次迭代来计算梯度下降 来收敛到<strong>全局最小值</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205846.png" alt></p>
<ul>
<li>相反地  正规方程法提供了一种求 θ 的解析解法 所以与其使用迭代算法   我们<strong>可以直接一次性求解θ的最优值</strong> 所以说基本上 一步就可以得到优化值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205857.png" alt></p>
<ul>
<li>正规方程法有一些优点 也有一些缺点 但是在我们讲解这个 和何时使用标准方程之前  让我们先对这个算法有一个直观的理解</li>
<li>我们举一个例子来解释这个问题 我们假设 有一个非常简单的代价函数 <code>J(θ)</code> 它就是一个<code>实数 θ</code> 的函数  所以现在 假设 <strong>θ 只是一个标量</strong> 或者说 θ 只有一行 它是一个数字 不是向量 假设我们的<code>代价函数 J</code> 是这个实参数 θ 的二次函数 所以 J(θ) 看起来是这样的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205908.png" alt></p>
<ul>
<li>那么如何最小化一个二次函数呢? 对于那些了解一点微积分的同学来说 你可能知道 <code>最小化的一个函数</code>的方法是 <strong>对它求导 并且将导数置零</strong>  所以对 J 求关于 θ 的导数 我不打算推导那些公式 你把那个导数置零 这样你就可以求得  使得 J(θ) 最小的 θ 值 这是数据为实数的 一个比较简单的例子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205924.png" alt></p>
<ul>
<li>在这个问题中 我们感兴趣的是 <strong>θ不是一个实数的情况 它是一个<code>n+1维</code>的参数向量</strong>  并且 <code>代价函数 J</code> 是这个向量的函数  也就是 θ0 到 θm 的函数 一个代价函数看起来是这样 像右边的这个平方代价函数 我们如何最小化这个代价函数J? 实际上 微积分告诉我们一种方法  <strong>对每个参数 θ 求 J 的偏导数  然后把它们全部置零 如果你这样做 并且求出θ0 θ1 一直到θn的值</strong> 这样就能得到能够最小化代价函数 J 的 θ 值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205935.png" alt></p>
<ul>
<li><p>如果你真的做完微积分和求解参数 θ0 到 θn   你会发现这个偏微分最终可能很复杂  接下来我在视频中要做的 实际上不是遍历所有的偏微分 因为这样太久太费事 我只是想告诉你们 你们想要实现这个过程所需要知道内容  这样你就可以解出 偏导数为0时 θ的值  换个方式说 或者等价地 这个 θ 能够使得代价函数 J(θ) 最小化  我发现可能只有熟悉微积分的同学 比较容易理解我的话 所以 如果你不了解 或者不那么了解微积分 也不必担心 我会告诉你 要实现这个算法并且使其正常运行 你所需的必要知识</p>
</li>
<li><p>举个例子 我想运行这样一个例子 假如说我有 m=4 个训练样本 假如说我有 m=4 个训练样本</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205946.png" alt></p>
<ul>
<li>为了实现<code>正规方程法</code> 我要这样做 看我的训练集 在这里就是这四个训练样本 在这种情况下 我们假设 这四个训练样本就是我的所有数据 我所要做的是 <strong>在我的训练集中加上一列对应额外特征变量的x0 就是那个取值永远是1的</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210007.png" alt></p>
<ul>
<li>接下来我要做的是 构建一个<code>矩阵 X</code> 这个矩阵<strong>基本包含了训练样本的所有特征变量</strong>  所以具体地说  这里有我所有的特征变量 我们要把这些数字 全部放到矩阵中 X 中 所以只是 每次复制一列的数据</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210034.png" alt></p>
<ul>
<li>我要对 y 做类似的事情  我要对我们将要预测的值 构建一个向量 像这样的 并且称之为<code>向量 y</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210111.png" alt></p>
<ul>
<li>所以 <code>X 会是一个 m*(n+1) 维矩阵</code> <code>y 会是一个 m 维向量</code>  其中 <code>m</code> 是<code>训练样本数量</code> <code>n</code> 是<code>特征变量数</code> n+1 是因为我加的这个额外的特征变量 x0  最后 如果你用矩阵 X 和向量 y 来计算这个 最后   <code>θ</code> 等于 <code>X 转置乘以 X 的逆 乘以 X 转置 乘以 y</code>  这样就得到能够使得代价函数最小化的 θ  幻灯片上的内容比较多 我讲解了这样一个数据组的一个例子 让我把这个写成更加通用的形式  在之后的视频中 我会仔细介绍这个方程</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210153.png" alt></p>
<ul>
<li>在一般情况下 假如我们有 m 个训练样本 x(1) y(1) 直到 x(m) y(m) n 个特征变量 所以每一个训练样本 xi 可能看起来像一个向量 像这样一个 n+1 维特征向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210255.png" alt></p>
<ul>
<li>我要构建矩阵 X 的方法  也被称为<code>设计矩阵</code> 如下所示  每个训练样本给出一个这样的特征向量 也就是说 这样的 n+1 维向量 我构建我的设计矩阵 X 的方法 就是构建这样的矩阵 接下来我要做的是将 取第一个训练样本 也就是一个向量 取它的转置 它最后是这样 扁长的样子 让 x1 转置作为我设计矩阵的第一行 然后我要把我的 第二个训练样本 x2 进行转置 让它作为 X 的第二行  以此类推 直到最后一个训练样本 取它的转置作为矩阵 X 的最后一行  这样矩阵 X 就是一个 m*(n+1) 维矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210323.png" alt></p>
<ul>
<li>举个具体的例子 假如我只有一个特征变量 就是说除了 x0 之外只有一个特征变量  而 x0 始终为1 所以如果我的特征向量 xi等于1 也就是x0 和某个实际的特征变量 比如说房屋大小 那么我的设计矩阵 X 会是这样 第一行 就是这个的转置  所以最后得到1 然后 x(1)1 对于第二行 我们得到1 然后 x(1)2  这样直到1 然后 x(1)m 这就会是一个 <code>m*2</code> 维矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210426.png" alt></p>
<ul>
<li>所以 <strong>这就是如何构建矩阵X 和向量y</strong>  有时我可能会在上面画一个箭头  来表示这是一个向量 但很多时候 我就只写y 是一样的 向量y 是这样求得的 把所有标签 所有训练集中正确的房子价格  放在一起 得到一个 m 维向量 y</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210443.png" alt></p>
<ul>
<li><strong>最后 构建完矩阵 X 和向量 y 我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到θ</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210456.png" alt></p>
<ul>
<li>我现在就想确保你明白这个等式  并且知道如何实现它 所以具体来说 什么是 X 的转置乘以 X 的逆？ <strong>X的转置 乘以 X的逆 是 X转置 乘以X的逆矩阵</strong> 具体来说 如果你令A等于 X转置乘以X  X的转置是一个矩阵  阵 我们把这个矩阵称为 A 那么 X转置乘以X的逆 就是矩阵 A 的逆  也就是 <code>1/A</code> 这就是计算过程 先计算 X转置乘以X 然后计算它的逆</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210506.png" alt></p>
<ul>
<li>我们还没有谈到Octave 我们将在之后的视频中谈到这个 但是在 Octave 编程语言  或者类似的 MATLAB 编程语言里  计算这个量的命令是基本相同的 X转置 乘以X的逆 乘以X转置 乘以y 的代码命令如下所示</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210525.png" alt></p>
<ul>
<li>在 Octave 中 <code>X’</code> 表示 <code>X 转置</code>  这个用红色框起来的表达式 计算的是 X 转置乘以 X  <code>函数 pinv</code> 是用来计算<code>逆矩阵</code>的函数 所以这个计算 X转置 乘以X的逆  然后乘以X转置 再乘以y  这样就算完了这个式子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210536.png" alt></p>
<ul>
<li>我没有证明这个式子 尽管我并不打算这么做 但是数学上是可以证明的 这个式子会给出最优的 θ 值  就是说如果你令 θ 等于这个 就是说如果你令 θ 等于这个 这个 θ 值会最小化这个线性回归的代价函数 J(θ)</li>
<li>最后一点 在之前视频中我提到<code>特征变量归一化</code> 和<code>让特征变量在相似的范围内</code>的想法 将所有的值归一化在类似范围内 <strong>如果你使用正规方程法 那么就不需要归一化特征变量</strong> 实际上这是没问题的 如果某个特征变量 x1 在 0到1的区间  某个特征变量 x2 在0到1000的区间  某个特征变量x3 在0到10^-5的区间 然后如果使用正规方程法 这样就没有问题 不需要做特征变量归一化 <strong>但如果你使用梯度下降法  特征变量归一化就很重要</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210549.png" alt></p>
<ul>
<li>最后 你何时应该使用梯度下降法 而何时应该使用正规方程法呢？ 这里列举了一些它们的优点和缺点 假如你有 m 个训练样本和 n 个特征变量 <strong>梯度下降法的缺点之一</strong>就是 你需要选择学习速率 α 这通常表示需要运行多次 尝试不同的学习速率 α 然后找到运行效果最好的那个 所以这是一种额外的工作和麻烦 <strong>梯度下降法的另一个缺点是</strong> 它需要更多次的迭代 因为一些细节 计算可能会更慢 我们一会儿会看到更多的东西</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210615.png" alt></p>
<ul>
<li><p>至于正规方程 你不需要选择学习速率 α 所以就非常方便 也容易实现 你只要运行一下 通常这就够了 并且你也不需要迭代 所以不需要画出 $J(\theta)$ 的曲线  来检查收敛性或者采取所有的额外步骤 到目前为止 天平似乎倾向于正规方程法</p>
</li>
<li><p>这里列举一些正规方程法的缺点 和梯度下降法的优点 <strong>梯度下降法在有很多特征变量的情况下也能运行地相当好</strong>  所以即使你有上百万的特征变量 所以即使你有上百万的特征变量 你可以运行梯度下降法 并且通常很有效 它会正常的运行 相对地 正规方程法 为了求解参数θ 需要求解这一项  我们需要计算这项 <code>X转置乘以X的逆</code> 这个是一个 <code>n*n</code> 的矩阵 如果你有 n 个特征变量的话 因为如果你看一下 X转置乘以X 的维度 你可以发现他们的<code>积的维度</code>  X转置乘以X 是一个 <code>n*n</code> 的矩阵 其中 n是特征变量的数量  <strong>实现逆矩阵计算所需要的计算量  大致是矩阵维度的三次方 因此计算这个逆矩阵需要计算大致 n 的三次方</strong>  有时稍微比计算 n 的三次方快一些 但是对我们来说很接近 所以如果特征变量的数量 n 很大的话 那么计算这个量会很慢  实际上标准方程法会慢很多  因此<strong>如果 n 很大 我可能还是会使用梯度下降法 因为我们不想花费 n 的三次方的时间 但如果 n 比较小 那么标准方程法可能更好地求解参数 θ</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210624.png" alt></p>
<ul>
<li><strong>那么怎么叫大或者小呢？</strong> 那么 如果 n 是上百的  计算百位数乘百位数的矩阵 对于现代计算机来说没有问题 如果 n 是上千的 我还是会使用正规方程法 千位数乘千位数的矩阵做逆变换 对于现代计算机来说实际上是非常快的 但如果 n 上万 那么我可能会开始犹豫 上万乘上万维的矩阵作逆变换 会开始有点慢 此时我可能开始倾向于 梯度下降法 但也不绝对 n 等于一万 你可以 逆变换一个一万乘一万的矩阵 但如果 n 远大于此 我可能就会使用梯度下降法了 所以如果 n 等于10^6 有一百万个特征变量 那么做百万乘百万的矩阵的逆变换 就会变得非常费时间 在这种情况下我一定会使用梯度下降法 所以很难给出一个确定的值 来决定何时该换成梯度下降法  但是 对我来说通常是 <strong>在一万左右 我会开始考虑换成梯度下降法</strong>  或者我们将在以后讨论到的其他算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210638.png" alt></p>
<ul>
<li>总结一下 只要特征变量的数目并不大 正规方程是一个很好的 计算参数 θ 的替代方法 具体地说 只要特征变量数量小于一万  我通常使用正规方程法  而不使用梯度下降法 预告一下在之后的课程中我们要讲的  随着我们要讲的学习算法越来越复杂  例如 当我们讲到分类算法 像逻辑回归算法 我们会看到 实际上对于那些算法 并不能使用正规方程法 对于那些更复杂的学习算法 我们将不得不仍然使用梯度下降法  因此 梯度下降法是一个非常有用的算法 可以用在有大量特征变量的线性回归问题 或者我们以后在课程中 会讲到的一些其他的算法 因为 标准方程法不适合或者不能用在它们上   但对于这个特定的线性回归模型 正规方程法是一个 比梯度下降法更快的替代算法 所以 根据具体的问题 所以  以及你的特征变量的数量 这两算法都是值得学习的<h3 id="正规方程笔记"><a href="#正规方程笔记" class="headerlink" title="正规方程笔记"></a>正规方程笔记</h3></li>
</ul>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210651.png" alt></p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 。<br> 假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。<br>上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={X^{T}}X$，则：${{\left( {X^T}X \right)}^{-1}}={A^{-1}}$<br>以下表示数据为例：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210731.png" alt></p>
<p>即：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210742.png" alt></p>
<p>运用正规方程方法求解参数：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210754.png" alt></p>
<p>在 Octave 中，正规方程写作：</p>
<pre><code>pinv(X&#39;*X)*X&#39;*y</code></pre><p>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。</p>
<p>梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>需要选择学习率{% raw %}$\alpha${% endraw %}</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量{% raw %}$n${% endraw %}大时也能较好适用</td>
<td>需要计算{% raw %}${{\left( {{X}^{T}}X \right)}^{-1}}${% endraw %} 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为{% raw %}$O\left( {{n}^{3}} \right)${% endraw %}，通常来说当{% raw %}$n${% endraw %}小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody></table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数{% raw %}$\theta ${% endraw %}的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，<br>实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<p>正规方程的python实现：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">normalEqn</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>

   theta <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>X<span class="token punctuation">.</span>T@X<span class="token punctuation">)</span>@X<span class="token punctuation">.</span>T@y <span class="token comment" spellcheck="true">#X.T@X等价于X.T.dot(X)</span>

   <span class="token keyword">return</span> theta</code></pre>
<h2 id="正规方程及不可逆性"><a href="#正规方程及不可逆性" class="headerlink" title="正规方程及不可逆性"></a>正规方程及不可逆性</h2><p>在这段视频中谈谈正规方程 ( normal equation )，以及它们的不可逆性。<br>由于这是一种较为深入的概念，并且总有人问我有关这方面的问题，因此，我想在这里来讨论它，由于概念较为深入，所以对这段可选材料大家放轻松吧，也许你可能会深入地探索下去，并且会觉得理解以后会非常有用。但即使你没有理解正规方程和线性回归的关系，也没有关系。</p>
<p>我们要讲的问题如下：$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$</p>
<p>有些同学曾经问过我，当计算 $\theta$=<code>inv(X&#39;X ) X&#39;y</code> ，那对于矩阵$X'X$的结果是不可逆的情况咋办呢?<br>如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为<code>奇异</code>或<code>退化矩阵</code>。</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210822.png" alt></p>
<p>问题的重点在于$X'X$的不可逆的问题很少发生，在Octave里，如果你用它来实现$\theta$的计算，你将会得到一个正常的解。在Octave里，有两个函数可以求解矩阵的逆，一个被称为<code>pinv()</code>，另一个是<code>inv()</code>，这两者之间的差异是些许计算过程上的，一个是所谓的<code>伪逆</code>，另一个被称为<code>逆</code>。使用<code>pinv()</code> 函数可以展现数学上的过程，这将计算出$\theta$的值，即便矩阵$X'X$是不可逆的。</p>
<p>在<code>pinv()</code> 和 <code>inv()</code> 之间，又有哪些具体区别呢 ?</p>
<p>其中<code>inv()</code> 引入了先进的数值计算的概念。</p>
<p>出现不可逆的一般有两个原因,第一个原因是:<strong>如果不知何故,再你的学习问题中,你有多余的功能</strong> 例如，在预测住房价格时，如果${x_{1}}$是以英尺为尺寸规格计算的房子，${x_{2}}$是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺 ( 四舍五入到两位小数 )，这样，你的这两个特征值将始终满足约束：${x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}$。<br>实际上，你可以用这样的一个线性方程，来展示那两个相关联的特征值，矩阵<code>X&#39;X</code>将是不可逆的。</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210836.png" alt></p>
<p>第二个原因是，<strong>在你想用大量的特征值，尝试实践你的学习算法的时候，可能会导致矩阵$X'X$的结果是不可逆的</strong>。<br>具体地说，<strong>在$m$小于或等于n的时候</strong>，例如，有$m$等于10个的训练样本也有$n$等于100的特征数量。要找到适合的$(n +1)$ 维参数矢量$\theta$，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有10个样本，以适应这100或101个参数，数据还是有些少。</p>
<p>稍后我们将看到，如何使用小数据样本以得到这100或101个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当$m$比$n$小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。</p>
<p>总之当你发现的矩阵$X'X$的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，我会建议你这么做。</p>
<p>首先，看特征值里是否有一些多余的特征，像这些${x_{1}}$和${x_{2}}$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。<br>如果矩阵$X'X$是不可逆的，（通常来说，不会出现这种情况），如果在Octave里，可以用伪逆函数<code>pinv()</code> 来实现。这种使用不同的线性代数库的方法被称为伪逆。即使$X'X$的结果是不可逆的，但算法执行的流程是正确的。总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注${X^{T}}X$是不可逆的。</p>
<p><strong>增加内容：</strong></p>
$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ 的推导过程：

$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$
<p>其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$</p>
<p>将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{2}}$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换</p>
$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{T}}\left( X\theta -y \right)$

<p>​     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}-{{y}^{T}} \right)\left(X\theta -y \right)$</p>
<p>​     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}X\theta -{{\theta}^{T}}{{X}^{T}}y-{{y}^{T}}X\theta -{{y}^{T}}y \right)$</p>
<p>接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:</p>
$\frac{dAB}{dB}={{A}^{T}}$

$\frac{d{{X}^{T}}AX}{dX}=2AX$

<p>所以有:</p>
$\frac{\partial J\left( \theta  \right)}{\partial \theta }=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{{X}^{T}}y -0 \right)$

<p>​           $={{X}^{T}}X\theta -{{X}^{T}}y$</p>
<p>令$\frac{\partial J\left( \theta  \right)}{\partial \theta }=0$,</p>
<p>则有$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$</p>

            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://www.fengwenhua.top" rel="external nofollow noreferrer">冯文华</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://www.fengwenhua.top/2018/11/15/di-si-zhang-duo-bian-liang-xian-xing-hui-gui-linear-regression-with-multiple-variables/">https://www.fengwenhua.top/2018/11/15/di-si-zhang-duo-bian-liang-xian-xing-hui-gui-linear-regression-with-multiple-variables/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://www.fengwenhua.top" target="_blank">冯文华</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Linear-Regression/">
                                    <span class="chip bg-color">Linear-Regression</span>
                                </a>
                            
                                <a href="/tags/多变量线性回归/">
                                    <span class="chip bg-color">多变量线性回归</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechatpay.jpg" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    

    
        <link rel="stylesheet" href="/libs/gitment/gitment-default.css">
<link rel="stylesheet" href="/css/gitment.css">

<div class="gitment-card card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitment-content" class="card-content"></div>
</div>

<script src="https://billts.site/js/gitment.js"></script>
<script>
var gitment = new Gitment({
    id: 'Wed Nov 14 2018 23:22:59 GMT+0000',
    owner: 'fengwenhua',
    repo: 'fengwenhua.github.io',
    oauth: {
        client_id: '487b675438c149b5818b',
        client_secret: '501477eec1e2ed9f27751d02281b5834021d6c7b'
    }
});

gitment.render('gitment-content');
</script>

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2018/11/15/ubuntu-pei-zhi/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="Ubuntu配置">
                        
                        <span class="card-title">Ubuntu配置</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            必备基础更新sudo apt update
sudo apt upgrade
sudo apt dist-upgrade
软件安装软件包里有的软件(一般都是这个)sudo apt install 软件名
.deb结尾的文件sudo dpkg
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2018-11-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Linux/" class="post-category">
                                    Linux
                                </a>
                            
                            <a href="/categories/Linux/Ubuntu/" class="post-category">
                                    Ubuntu
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/ubuntu配置/">
                        <span class="chip bg-color">ubuntu配置</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2018/11/15/di-san-zhang-xian-xing-dai-shu-hui-gu-linear-algebra-review/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/4.jpg" class="responsive-img" alt="第三章 线性代数回顾(Linear Algebra Review)">
                        
                        <span class="card-title">第三章 线性代数回顾(Linear Algebra Review)</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Matrices and Vectors（矩阵和向量）
我们先复习一下线性代数的知识 在这段视频中 我会向大家介绍矩阵和向量的概念


矩阵是指 由数字组成的矩形阵列 并写在方括号中间 例如 屏幕中所示的一个矩阵 先写一个左括号 然后是一些
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2018-11-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/机器学习/" class="post-category">
                                    机器学习
                                </a>
                            
                            <a href="/categories/机器学习/机器学习入门/" class="post-category">
                                    机器学习入门
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/线性代数/">
                        <span class="chip bg-color">线性代数</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>

    
<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>


    <footer class="page-footer bg-color">
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2018</span>
            <a href="https://www.fengwenhua.top" target="_blank">冯文华</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">146k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/fengwenhua" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:807296772@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=807296772" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 807296772" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140817753-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'UA-140817753-1');
</script>


    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    
    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
